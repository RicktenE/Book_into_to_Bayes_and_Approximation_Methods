[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to Bayesian inference and approximation methods in R (work in progress)",
    "section": "",
    "text": "Preface\nThis file describes basic use of R for Bayesian statistical analysis and approximation methods. The importance on the choices of prior are shown. The influence of different shapes of priors are shown. The file concludes with approximation methods. It is a work in progress and the later chapters (to be continued) might still be added for completion. This is a collection of works based on a collaboration between Rick ten Eikelder and Duco Veen."
  },
  {
    "objectID": "Intro_to_Bayesian_inference_.html",
    "href": "Intro_to_Bayesian_inference_.html",
    "title": "Intro to Bayesian inference",
    "section": "",
    "text": "Introduction to Bayesian Inference with R\nIn this introduction, we will explore Bayesian inference in R. Bayesian inference is a method of statistical inference in which Bayes’ theorem is used to update the probability for a hypothesis as more evidence or information becomes available. This method allows us to incorporate prior knowledge into our analysis and make more informed decisions.\nWe will begin by discussing the basics of Bayesian inference and Bayes’ theorem. Next, we will learn about the various types of prior distributions that can be used in Bayesian analysis. Finally, we will walk through several examples of Bayesian inference in R using the rstan package for Bayesian modeling.\nBefore diving in, it is important to note that some understanding of probability and statistics is recommended for following along with this text. Additionally, the rstan package requires the Rtools to be installed on your system.\nLet’s get started!\n\n\nBayes’ Theorem\nBayes’ theorem is the foundation of Bayesian inference. It states that the probability of a hypothesis (H) given some data (D) is proportional to the probability of the data given the hypothesis (D|H) multiplied by the prior probability of the hypothesis (H).\nMathematically, it can be represented as:\n\\[P(H|D) = \\frac{P(D|H) * P(H)}{P(D)}\\]\nWhere:\n\\(P(H|D)\\) is the posterior probability of the hypothesis given the data. \\(P(D|H)\\) is the likelihood of the data given the hypothesis. \\(P(H)\\) is the prior probability of the hypothesis. \\(P(D)\\) is the marginal likelihood, also known as the evidence. The posterior probability, \\(P(H|D)\\), represents our updated belief about the hypothesis after taking into account the data. The prior probability, \\(P(H)\\), represents our initial belief about the hypothesis before taking into account the data. The likelihood, \\(P(D|H)\\), represents how likely the data is given the hypothesis. The marginal likelihood, \\(P(D)\\), is a normalizing constant that ensures the posterior probability is a valid probability distribution.\nIn Bayesian inference, we use Bayes’ theorem to update our belief about a hypothesis by incorporating new data. The process of updating our belief is known as updating the prior to the posterior.\n\n\nPrior Distribution\nIn Bayesian inference, prior distribution is used to express our initial belief about the parameters of the model before observing the data. Prior distributions can be chosen based on previous knowledge, expert opinion, or other sources of information.\nThere are different types of prior distributions that can be used in Bayesian analysis, including:\nNon-informative prior: A non-informative prior is a prior distribution that has minimal information about the parameters of the model. Examples of non-informative priors include the uniform distribution and the improper prior.\nConjugate prior: A conjugate prior is a prior distribution that belongs to the same family as the likelihood function. When a conjugate prior is used, the posterior distribution also belongs to the same family as the prior and the likelihood.\nEmpirical prior: An empirical prior is a prior distribution that is based on historical data. Empirical priors are often used when there is no prior knowledge available about the parameters of the model.\nInformative prior: An informative prior is a prior distribution that incorporates a significant amount of information about the parameters of the model. Informative priors are often used when there is a lot of prior knowledge available about the parameters of the model.\nIt is important to note that the choice of prior distribution can have a significant impact on the results of Bayesian analysis, this is demonstrated in the next parts of this chapter. In general, it is a good practice to choose a prior distribution that reflects our current knowledge or belief about the parameters of the model.\n\n\nBayesian Inference in R\nIn R, the rstan package is widely used for Bayesian modeling. rstan is an interface to the Stan C++ library for Bayesian inference and it allows users to fit Bayesian models using Hamiltonian Monte Carlo (HMC) or Variational Inference (VI).\nTo start using rstan, we first need to install the package and its dependencies. The installation of the rstan package requires the Rtools to be installed on the system.\n\n# https://blog.mc-stan.org/2022/04/26/stan-r-4-2-on-windows/\n# install.packages(\"rstan\")\n\n# Remove currently installed rstan andd StanHeaders packages\n# If you have not installed these packages yet, proceed to step 2.\n\n# remove.packages(c(\"rstan\", \"StanHeaders\"))\n# Install from the Stan R package repository\n\n# install.packages(\"rstan\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\n\nOnce the package is installed, we can begin by specifying the model we want to fit using the Stan language. The Stan language is a probabilistic programming language that is used to specify Bayesian models. It allows us to define the likelihood function, prior distributions, and the parameters of the model.\nIn order to fit a Bayesian model using rstan, we need to do the following steps:\nFit the model to the data using the sampling() function. Here is an example of how to fit a simple linear regression model using rstan:\n\nlibrary(StanHeaders)\nlibrary(ggplot2)\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\n# Step 1: Write the model in Stan language\nmodel &lt;- \"\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] x;\n  vector[N] y;\n}\nparameters {\n  real alpha;\n  real beta;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  y ~ normal(alpha + beta * x, sigma);\n}\n\"\n\n# Step 2: Compile the model\nmodel &lt;- stan_model(model_code = model)\n\n# Step 3: Fit the model to the data\ndata &lt;- list(N = 100, x = rnorm(100), y = rnorm(100))\nfit &lt;- sampling(model, data = data)\n\nIn this example, we specified the model in Stan language as a string and passed it to the stan_model() function. The model is defined by three parameters, alpha, beta, and sigma, and a likelihood function, which is a normal distribution. We also specified the data that we want to fit the model to.\nAfter fitting the model, we can extract the posterior samples using the extract() function and use them to make inferences about the parameters of the model.\n\nposterior_sample &lt;- extract(fit)\nhist(posterior_sample$alpha)\n\n\n\nhist(posterior_sample$beta)\n\n\n\n\nWe can also use the summary() function to get a summary of the posterior samples.\n\nsummary(fit)\n\n$summary\n              mean     se_mean         sd        2.5%          25%          50%\nalpha  -0.05484634 0.001540790 0.09877400  -0.2480043  -0.12208209  -0.05460930\nbeta    0.03900751 0.001503306 0.09666790  -0.1510422  -0.02485507   0.03853799\nsigma   0.99442561 0.001222302 0.07135842   0.8683019   0.94409887   0.99128136\nlp__  -48.67436299 0.028425827 1.22736502 -51.8347498 -49.24862041 -48.36354920\n               75%       97.5%    n_eff      Rhat\nalpha   0.01152634   0.1361985 4109.586 0.9999308\nbeta    0.10374643   0.2306704 4134.948 1.0005715\nsigma   1.03967170   1.1476783 3408.268 1.0001592\nlp__  -47.75664948 -47.2666562 1864.324 1.0006563\n\n$c_summary\n, , chains = chain:1\n\n         stats\nparameter         mean         sd        2.5%          25%          50%\n    alpha  -0.05709371 0.09757981  -0.2471315  -0.12310819  -0.05759117\n    beta    0.03651528 0.09684627  -0.1525438  -0.02310176   0.03794865\n    sigma   0.99284436 0.07162529   0.8664163   0.94162997   0.99037988\n    lp__  -48.66354424 1.23197982 -52.0483587 -49.19342552 -48.36879549\n         stats\nparameter          75%       97.5%\n    alpha   0.01160497   0.1263622\n    beta    0.09941187   0.2229508\n    sigma   1.03775770   1.1465104\n    lp__  -47.77041488 -47.2629841\n\n, , chains = chain:2\n\n         stats\nparameter         mean         sd        2.5%          25%          50%\n    alpha  -0.05770044 0.09715318  -0.2460799  -0.12312813  -0.05808516\n    beta    0.03885034 0.09578885  -0.1538171  -0.02749813   0.03881504\n    sigma   0.99517703 0.06922725   0.8750465   0.94835190   0.99095984\n    lp__  -48.62225025 1.20378620 -51.7135527 -49.22311051 -48.29251974\n         stats\nparameter           75%       97.5%\n    alpha   0.006311798   0.1363272\n    beta    0.104804890   0.2299949\n    sigma   1.038284789   1.1461003\n    lp__  -47.727367300 -47.2481885\n\n, , chains = chain:3\n\n         stats\nparameter         mean         sd        2.5%          25%          50%\n    alpha  -0.05325436 0.10292779  -0.2530522  -0.12364880  -0.05147409\n    beta    0.04339726 0.09471326  -0.1268470  -0.02442053   0.03847646\n    sigma   0.99460010 0.07310278   0.8604334   0.94472536   0.99165317\n    lp__  -48.71891620 1.27211939 -51.7263412 -49.34043001 -48.40490527\n         stats\nparameter          75%       97.5%\n    alpha   0.01383604   0.1500638\n    beta    0.10746961   0.2351478\n    sigma   1.04007135   1.1519796\n    lp__  -47.80104359 -47.2899148\n\n, , chains = chain:4\n\n         stats\nparameter         mean         sd        2.5%          25%          50%\n    alpha  -0.05133684 0.09732288  -0.2443792  -0.11662012  -0.04880835\n    beta    0.03726715 0.09926189  -0.1640099  -0.02391606   0.03925483\n    sigma   0.99508095 0.07150700   0.8682363   0.94189635   0.99163019\n    lp__  -48.69274127 1.19995810 -51.8372455 -49.26970243 -48.39879817\n         stats\nparameter          75%       97.5%\n    alpha   0.01337844   0.1342808\n    beta    0.10232022   0.2284402\n    sigma   1.04341637   1.1462391\n    lp__  -47.76476396 -47.2884130\n\n\nSeveral different priors can be implemented in the ‘rstan’ package. With a normal distribution we can specify weakly informative priors or strong priors. This will be examined in the next parts of this chapter. Other options are a custom prior or a Dirichlet prior.\n\n\nModel Checking\nAfter fitting a Bayesian model, it is important to check the model’s fit to the data. This process is known as model checking. Model checking helps us to ensure that the model is a good fit to the data and that the posterior samples are representative of the true posterior distribution.\nThere are several methods for model checking in Bayesian inference. Some of the most common methods include:\nTrace plots: A trace plot shows the progression of the Markov chain for each parameter. It helps to check for convergence and to identify any problems with the sampling process.\n\n# Trace plot for alpha\ntraceplot(fit, pars = \"alpha\")\n\n\n\n# Trace plot for beta\ntraceplot(fit, pars = \"beta\")\n\n\n\n\nHere’s how to interpret trace plots:\n-Each chain corresponds to a different MCMC chain. -The y-axis represents the parameter values. -The x-axis represents the iteration number. -Chains have converged: The chains should show a stable behavior, indicating that they have converged to the target distribution. -Random walk behavior: The chains should exhibit a random walk-like pattern without strong trends or patterns.\nAutocorrelation plots: An autocorrelation plot shows the correlation between the samples at different lags. It helps to check for independence of the samples and to identify any problems with the mixing of the Markov chain.\n\n# Autocorrelation plot for alpha\nalpha_samples &lt;- extract(fit)$alpha\nautocorr_alpha &lt;- acf(alpha_samples, lag.max = 20)\nplot(autocorr_alpha)\n\n\n\n\nDensity plots: A density plot shows the distribution of the posterior samples for each parameter. It helps to check for unimodality and to identify any problems with the choice of prior distribution.\n\n# Density plot for alpha\ndensity_plot_alpha &lt;- ggplot(data.frame(alpha = alpha_samples), aes(x = alpha)) +\n  geom_density() +\n  labs(title = \"Density Plot for Alpha\") +\n  theme_minimal()\n\nprint(density_plot_alpha)\n\n\n\n\n\n\nMaking Inferences from Posterior Samples\nOnce we have performed model checking and ensured that the model is a good fit to the data, we can use the posterior samples to make inferences about the parameters of the model .\nOne way to make inferences is by calculating summary statistics of the posterior samples, such as the mean, median, and credible intervals. The mean and median of the posterior samples provide a point estimate of the parameter, while the credible intervals provide a range of plausible values.\nAnother way to make inferences is by visualizing the posterior samples using plots such as histograms and kernel density plots. These plots can help to understand the shape of the posterior distribution and the uncertainty of the estimates.\n\n# Perform model checking and ensure a good fit to the data\n\n# Calculate summary statistics\nsummary_stats &lt;- summary(fit)\n\n# Visualize posterior distribution using histograms\nhist_plot_alpha &lt;- ggplot(data.frame(alpha = extract(fit)$alpha), aes(x = alpha)) +\n  geom_histogram(binwidth = 0.1, fill = \"blue\", color = \"black\") +\n  labs(title = \"Histogram of Posterior Samples for Alpha\")+\n  theme_minimal()\n\nprint(hist_plot_alpha)\n\n\n\n# Visualize posterior distribution using kernel density plots\ndensity_plot_beta &lt;- ggplot(data.frame(beta = extract(fit)$beta), aes(x = beta)) +\n  geom_density(fill = \"green\", color = \"black\") +\n  labs(title = \"Kernel Density Plot of Posterior Samples for Beta\")+\n  theme_minimal()\n\nprint(density_plot_beta)\n\n\n\n# Simulate from the model using posterior samples\nnum_simulations &lt;- 1000\nsimulated_data &lt;- data.frame(x = rnorm(num_simulations))\nsimulated_predictions &lt;- vector(\"list\", num_simulations)\n\nfor (i in 1:num_simulations) {\n  simulated_predictions[[i]] &lt;- rpois(1, exp(summary_stats$summary[1, \"mean\"] + summary_stats$summary[2, \"mean\"] * simulated_data$x[i]))\n}\n\n\n\nConclusion\nIn this guide, we will discus the basics of Bayesian inference in R using the rstan package. We have shown how to specify a model in Stan language, how to fit the model to the data. Bayesian inference is a powerful tool for statistical modeling and can be applied to a wide range of problems. The rstan package provides a flexible and efficient way to perform Bayesian inference in R. It allows us to specify complex models, perform model checking, and make inferences from the posterior samples with ease."
  },
  {
    "objectID": "Intro_to_The_Bayes_filter_.html#load-libraries-needed",
    "href": "Intro_to_The_Bayes_filter_.html#load-libraries-needed",
    "title": "1  The Bayes filter",
    "section": "1.1 load libraries needed",
    "text": "1.1 load libraries needed\n\nlibrary(rstanarm)\nlibrary(ggplot2)\nlibrary(bayesplot)\nlibrary(blavaan)\nlibrary(dplyr)\nlibrary(magick)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(ggridges)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(rsvg)\ntheme_set(bayesplot::theme_default())\ntheme_set(theme_bw())"
  },
  {
    "objectID": "Intro_to_The_Bayes_filter_.html#create-sample",
    "href": "Intro_to_The_Bayes_filter_.html#create-sample",
    "title": "1  The Bayes filter",
    "section": "1.2 Create sample",
    "text": "1.2 Create sample\nSimulate the sample from a normal distribution with mu = 15, std = 2 set in data.frame for rstanarm to read the data.\n\nset.seed(1234)\nn &lt;- 1000\nx0 &lt;- rnorm(n, 15, 4)\nb0 = 1\ny = b0 * x0\ndf = data.frame(y, x0)"
  },
  {
    "objectID": "Intro_to_The_Bayes_filter_.html#fit-the-simple-model",
    "href": "Intro_to_The_Bayes_filter_.html#fit-the-simple-model",
    "title": "1  The Bayes filter",
    "section": "1.3 Fit the simple model",
    "text": "1.3 Fit the simple model\nHere we have a normally distributed prior, centered around 1 with a variance of 3.\nWe are testing if the model can find the mean of 15 from the population of which X0 is sampled. In a more complicated model it is also possible to check for b1, b2 etc for explanatory variables x1, x2 etc. In this simulation we purely look at the influence of the changes in prior to the posterior.\n\nfit &lt;- stan_glm(y ~ 1,\n                data = df,\n                prior_intercept = normal(c(1), 3),\n                cores = parallel::detectCores(),\n                seed = 1234)\n\nposterior1 &lt;- as.array(fit)"
  },
  {
    "objectID": "Intro_to_The_Bayes_filter_.html#evaluate-the-simple-model",
    "href": "Intro_to_The_Bayes_filter_.html#evaluate-the-simple-model",
    "title": "1  The Bayes filter",
    "section": "1.4 Evaluate the simple model",
    "text": "1.4 Evaluate the simple model\nThere are several ways to evaluate this model. A first insight can be generated by looking at the summary of the model. This gives the most basic information on the posterior distributions. Each variable of the model has its own posterior distribution, as it starts with a prior distribution. With the function mcmc_areas() the posterior distributions around the estimated variables are plotted in the same graph. Another way of showing these posterior distributions is by using mcmc_hist(). This function takes samples of the posterior. It is also possible to show the Credible intervals in the mcmc_areas() function.\n\nsummary(fit)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      y ~ 1\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 1000\n predictors:   1\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 14.9    0.1 14.7  14.9  15.0 \nsigma        4.0    0.1  3.9   4.0   4.1 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 14.9    0.2 14.6  14.9  15.1 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  2399 \nsigma         0.0  1.0  2877 \nmean_PPD      0.0  1.0  3028 \nlog-posterior 0.0  1.0  1630 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\ncolor_scheme_set(\"blue\")\nmcmc_areas(fit, \n           pars = c( \"(Intercept)\", \"sigma\"),\n           prob = 0.8, # 80% intervals\n           prob_outer = 0.999, # 99.9% of the total distribution\n           point_est = \"mean\")\n\n\n\nmcmc_hist(posterior1)"
  },
  {
    "objectID": "Intro_to_The_Bayes_filter_.html#repeat-all-of-this-above-with-a-small-sample-and-see-how-this-influences-the-model",
    "href": "Intro_to_The_Bayes_filter_.html#repeat-all-of-this-above-with-a-small-sample-and-see-how-this-influences-the-model",
    "title": "1  The Bayes filter",
    "section": "1.5 Repeat all of this above with a small sample and see how this influences the model",
    "text": "1.5 Repeat all of this above with a small sample and see how this influences the model\nIn the above simulation it is shown that the variance and mean of the population is found in the posterior. In the above example we have a data set containing 500 values and a rather sharp prior around 1 with a variance of 3. Regardless of the sharp prior, the sample size was large enough to pull the posterior to the data. In the next example we will look what happens to the posterior if we take a much smaller data set but the same prior.\n\nset.seed(1234)\nn &lt;- 8\nx0_2 &lt;- rnorm(n, 15, 2)\ny2 = 1 * x0_2\ndf = data.frame(y2, x0_2)\nfit2 &lt;- stan_glm(y2 ~ 1, \n                 data = df,\n                 prior_intercept = normal(c(1), 3),\n                 # prior = IG()\n                 cores = parallel::detectCores(),\n                 seed = 1234)\n\nposterior2 &lt;- as.array(fit2)\n\ncolor_scheme_set(\"red\")\nmcmc_areas(fit2, \n           pars = c( \"(Intercept)\", \"sigma\"),\n           prob = 0.8, # 80% intervals\n           prob_outer = 0.99, # 99%\n           point_est = \"mean\")\n\n\n\nmcmc_hist(posterior2)\n\n\n\n\nClearly the model can predict less good with a smaller sample size. This is visible in the mean and sigma. The distributions are skewed and the means are on the wrong position."
  },
  {
    "objectID": "Intro_to_The_Bayes_filter_.html#returning-to-the-sample-size-of-500-but-introduce-an-error-term-in-the-model.",
    "href": "Intro_to_The_Bayes_filter_.html#returning-to-the-sample-size-of-500-but-introduce-an-error-term-in-the-model.",
    "title": "1  The Bayes filter",
    "section": "1.6 Returning to the sample size of 500, but introduce an error term in the model.",
    "text": "1.6 Returning to the sample size of 500, but introduce an error term in the model.\n\nn &lt;- 500\nx0 &lt;- rnorm(n, 15, 2)\nb0 = 1\ne &lt;- rnorm(n, 0, 1)\ny_e = 1 * x0 + e\ndf_e = data.frame(y, y_e, x0)\n\nfit_e &lt;- stan_glm(y_e ~ 1, \n                  data = df_e, \n                  prior = normal(c(1), 3), \n                  cores = parallel::detectCores(), \n                  seed = 1234)\n\nposterior_error &lt;- as.array(fit_e)\n\nsummary(fit_e)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      y_e ~ 1\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 1000\n predictors:   1\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 15.0    0.1 14.9  15.0  15.1 \nsigma        2.3    0.1  2.3   2.3   2.4 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 15.0    0.1 14.8  15.0  15.1 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  3069 \nsigma         0.0  1.0  2956 \nmean_PPD      0.0  1.0  3511 \nlog-posterior 0.0  1.0  1670 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nsummary(fit)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      y ~ 1\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 1000\n predictors:   1\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 14.9    0.1 14.7  14.9  15.0 \nsigma        4.0    0.1  3.9   4.0   4.1 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 14.9    0.2 14.6  14.9  15.1 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  2399 \nsigma         0.0  1.0  2877 \nmean_PPD      0.0  1.0  3028 \nlog-posterior 0.0  1.0  1630 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\ncolor_scheme_set(\"blue\")\nmcmc_areas(fit, \n           pars = c( \"(Intercept)\", \"sigma\"),\n           prob = 0.8, # 80% intervals\n           prob_outer = 0.99, # 99%\n           point_est = \"mean\")\n\n\n\nmcmc_hist(posterior_error)\n\n\n\n\nThis error is visible in the mean and SD of the intercept and Sigma. So we have seen here that the size of the data and the chosen model both influence the posterior distribution. Let’s assume, for simulation purposes, that a very simple model suffices. We leave out the error and go back to a sample size of n = 500."
  },
  {
    "objectID": "Intro_to_The_Bayes_filter_.html#investigate-the-influence-of-the-prior.-keeping-the-data-and-the-model-the-same.",
    "href": "Intro_to_The_Bayes_filter_.html#investigate-the-influence-of-the-prior.-keeping-the-data-and-the-model-the-same.",
    "title": "1  The Bayes filter",
    "section": "1.7 Investigate the influence of the prior. Keeping the data and the model the same.",
    "text": "1.7 Investigate the influence of the prior. Keeping the data and the model the same.\n\nn &lt;- 500\nx0 &lt;- rnorm(n,15, 2) \nb0 = 1 \ny = b0 * x0\ndf = data.frame(y, x0) \n\nfit_wide &lt;- stan_glm(y ~ 1, \n                     data = df, \n                     prior_intercept = normal(1, 1E10), \n                     cores = parallel::detectCores(),\n                     seed = 1234)\n\nfit_far &lt;- stan_glm(y ~ 1, \n                         data = df, \n                         prior_intercept = normal(1E5, 1E2), \n                         cores = parallel::detectCores(), \n                         seed = 1234)\n\nfit_far_wide &lt;- stan_glm(y ~ 1,\n                         data = df,\n                         prior_intercept = normal(1E5, 1E10),\n                         cores = parallel::detectCores(),\n                         seed = 1234)\n\n\nfit_far_sharp &lt;- stan_glm(y ~ 1,\n                         data = df,\n                         prior_intercept = normal(1E5,1E-5),\n                         cores = parallel::detectCores(),\n                         seed = 1234)\n\nposterior_wide &lt;- as.array(fit_wide)\nposterior_far &lt;- as.array(fit_far)\nposterior_far_sharp &lt;- as.array(fit_far_sharp)\nposterior_far_wide &lt;- as.array(fit_far_wide)\n\n\n# par(2,2)\ncolor_scheme_set(\"green\")\n# mcmc_areas(posterior_wide, \n#            pars = c(\"(Intercept)\", \"sigma\"),\n#            prob = 0.8, # 80% intervals\n#            prob_outer = 0.99, # 99%\n#            point_est = \"mean\")\n# \nmcmc_hist(posterior_wide)\n\n\n\n# \ncolor_scheme_set(\"blue\")\n# mcmc_areas(posterior_far, \n#            pars = c(\"(Intercept)\", \"sigma\"),\n#            prob = 0.8, # 80% intervals\n#            prob_outer = 0.99, # 99%\n#            point_est = \"mean\")\nmcmc_hist(posterior_far)\n\n\n\n# \n# mcmc_areas(posterior_far_sharp, \n#            pars = c(\"(Intercept)\", \"sigma\"),\n#            prob = 0.8, # 80% intervals\n#            prob_outer = 0.99, # 99%\n#            point_est = \"mean\")\nmcmc_hist(posterior_far_wide)\n\n\n\n# \n# mcmc_areas(posterior_far_wide, \n#            pars = c(\"(Intercept)\", \"sigma\"),\n#            prob = 0.8, # 80% intervals\n#            prob_outer = 0.99, # 99%\n#            point_est = \"mean\")\nmcmc_hist(posterior_far_sharp)\n\n\n\n\nGreat! here we see a clear pattern. When we use a very wide prior, the data is enough to help the model predict the parameters of the population correctly. This prior is non-informative.\nWhen we use a prior that is relatively far, mean at 10.000, but with a large variance of 100. The prior is weakly informative compared to the data. With this prior it is still possible for the model to find the correct parameters. Even though the mean of the prior is very far off.\nWhen we specify our prior relatively far but extremely wide, it approximates the prior we specified as a wide prior. Both priors are non-informative and with either prior the model can still find the correct parameters.\nHowever, when we specify a prior relatively far and sharp, the model cannot predict the correct parameters anymore. The data set is too small for the relative influence of the prior. The prior draws the posterior to the mean of the prior."
  },
  {
    "objectID": "The_Bayes_filter___Mean_influence_.html#mean-study",
    "href": "The_Bayes_filter___Mean_influence_.html#mean-study",
    "title": "3  Bayes filter - Mean influence",
    "section": "3.1 Mean study",
    "text": "3.1 Mean study\nLets make a dataframe of each iteration and put this in a list. Then use this list to create the plots later. The making of the plots in a different loop makes that the program is more easily adjustable. If we want to make changes in the plots we don’t have to run the data frame creation\n\n## create the sample (data)\nset.seed(1234)\nn &lt;- 100\nx0 &lt;- rnorm(n, 12, 2)\nb0 = 1\ny = b0 * x0\ndf = data.frame(y, x0)\n\n# Design the different priors -&gt; in this case only the variance changes\nmeans &lt;- c(12:50)\n# variance &lt;- c(1:3)\n\n# initialise the list for plotting the distributions later on\nlist_posterior &lt;-list()\nlist_prior &lt;- list()\n\n#index\ncount_i &lt;- 0\n\n#fit the model and produce the posterior, save both posterior and corresponding prior\nfor(i in means){\n  \n  count_i &lt;- count_i +1\n  list_prior[[count_i]] &lt;- as.array(rnorm(2000, i, 3))\n  fit_test &lt;- stan_glm(y ~ 1, \n                         data = df, \n                         prior_intercept = normal(c(i), 3), \n                         cores = parallel::detectCores(), \n                         seed = 1234)\n  \n  draws &lt;- as.data.frame(fit_test)\n  list_posterior[[count_i]] &lt;- draws$`(Intercept)`\n\n}\n\n## prep the data \n\nlist_dist &lt;- list()\n\nfor(index in 1:length(means)){\n\n  a &lt;- list_posterior[[index]]\n  \n  b &lt;- list_prior[[index]]\n  \n  c &lt;- y \n  \n  ab_ &lt;- append(a,b)\n  abc_ &lt;- append(ab_, c)\n  \n  df_dist &lt;- data.frame(abc_)\n  df_dist &lt;- data.frame(unlist(df_dist))\n  df_dist$group &lt;- c(rep('posterior', length(a)), rep('prior', length(b)), rep('sample', length(c)))\n  list_dist[[index]] &lt;- df_dist\n\n}\n\n\n3.1.1 Ridgeline plots - Create plots and .gif from the results\n\n## create a directory to which the images will be written\n\ndir_out &lt;- file.path(getwd(), \"post_prior_data_ridge_means\")\ndir.create(dir_out, recursive = TRUE)\n\n## Create the plots with ridgelines \n\nfor(j in 1:length(means)){\n  \n  ## Creating the plots\n  # p &lt;- ggplot(list_dist[[j]], aes(x = unlist.df_dist., y = group, fill = group)) +\n  #           stat_density_ridges(quantile_lines = TRUE, quantiles = 0.5) +\n  #           labs(x = \"Param value\", y = \"Prob\", title = means[j]) +\n  #           xlim(10, 55) +\n  #           theme(legend.position = \"none\")\n\n    \n  p &lt;-  ggplot(list_dist[[j]], aes(x = unlist.df_dist., y = group, fill = 0.5 - abs(0.5 - stat(ecdf)))) +\n            stat_density_ridges(geom = \"density_ridges_gradient\", calc_ecdf = TRUE) +\n            scale_fill_viridis_c(name = \"Tail probability\", direction = -1)+\n            theme(legend.position = \"none\") +\n            xlim(10, 55) +\n            labs(x = \"Param value\", y = \"Count\", title = means[j])\n  \n    fp &lt;- file.path(dir_out, paste(stringr::str_pad(j, 4, pad = \"0\"), \".png\", sep=\"\"))\n  \n    ggsave(plot = p,\n           filename = fp,\n           device = \"png\")\n}\n\n\n# list file names and read in\nimgs &lt;- list.files(dir_out, full.names = TRUE)\n# imgs &lt;- list.files(dir_out)\nimg_list &lt;- lapply(imgs, image_read)\n\n## join the images together\nimg_joined &lt;- image_join(img_list)\n\n## animate at 2 frames per second\nimg_animated &lt;- image_animate(img_joined, fps = 2, optimize = FALSE)\n\n\n## save to disk\nimage_write(image = img_animated,\n            path = \"post_prior_data_ridge_means.gif\")\n\n\n\n3.1.2 Histogram plots - Create the plots with histograms\n\ndir_out &lt;- file.path(getwd(), \"post_prior_data_hist_means\")\ndir.create(dir_out, recursive = TRUE)\n\nfor(j in 1:length(means)){\n  \n  ## Creating the plots\n  p &lt;- ggplot(list_dist[[j]], aes(x = unlist.df_dist., fill = group )) +\n            geom_histogram() +\n            # ylim(0, 0.8) +\n            xlim(10, 55) +\n            theme_light() +\n            labs(x = \"Param value\", y = \"Count\", title = means[j])\n  \n    fp &lt;- file.path(dir_out, paste(stringr::str_pad(j, 4, pad = \"0\"), \".png\", sep=\"\"))\n  \n    ggsave(plot = p,\n           filename = fp,\n           device = \"png\")\n}\n\n# list file names and read in\nimgs &lt;- list.files(dir_out, full.names = TRUE)\nimg_list &lt;- lapply(imgs, image_read)\n\n## join the images together\nimg_joined &lt;- image_join(img_list)\n\n## animate at 2 frames per second\nimg_animated &lt;- image_animate(img_joined, fps = 2, optimize = TRUE)\n\n\n## save to disk\nimage_write(image = img_animated,\n            path = \"post_prior_data_hist_means.gif\")\n\n\n\n3.1.3 Density plots - Create the plots with Density plots\n\ndir_out &lt;- file.path(getwd(), \"post_prior_data_dens_means\")\ndir.create(dir_out, recursive = TRUE)\n\nfor(j in 1:length(means)){\n  \n  ## Creating the plots\n  p &lt;- ggplot(list_dist[[j]], aes(x = unlist.df_dist., fill = group )) +\n            geom_density(alpha = .5) +\n            ylim(0, 0.8) +\n            xlim(10, 55) +\n            theme_light() +\n            labs(x = \"Param value\", y = \"Count\", title = means[j])\n  \n    fp &lt;- file.path(dir_out, paste(stringr::str_pad(j, 4, pad = \"0\"), \".png\", sep=\"\"))\n  \n    ggsave(plot = p,\n           filename = fp,\n           device = \"png\")\n}\n\n# list file names and read in\nimgs &lt;- list.files(dir_out, full.names = TRUE)\nimg_list &lt;- lapply(imgs, image_read)\n\n## join the images together\nimg_joined &lt;- image_join(img_list)\n\n## animate at 2 frames per second\nimg_animated &lt;- image_animate(img_joined, fps = 2, optimize = TRUE)\n\n\n## save to disk\nimage_write(image = img_animated,\n            path = \"post_prior_data_dens_means.gif\")\n\n\n\nplay_image &lt;- TRUE\nif(play_image){\n  ## view animated image\n  img_animated}"
  },
  {
    "objectID": "Intro_to_approximation_methods_.html",
    "href": "Intro_to_approximation_methods_.html",
    "title": "Intro to approximation methods",
    "section": "",
    "text": "In this chapter, we will explore several popular approximation methods implemented in R: the Gaussian sum filter, Kalman filter, particle filter (also known as sequential Monte Carlo), and the bootstrap filter. These methods are commonly used in a range of applications, including signal processing, robotics, and finance, to name a few.\nThe Gaussian sum filter is a technique used to approximate probability distributions that can be represented as a sum of Gaussian distributions. It is often used in signal processing to estimate the state of a system from noisy measurements. In Python and R, this technique is implemented using the Kalman filter, which is a widely-used algorithm for state estimation in linear dynamic systems.\nThe particle filter, or sequential Monte Carlo, is an alternative method for estimating the state of a system when the underlying distribution cannot be represented by a simple model. It works by approximating the true distribution using a set of weighted particles. In both Python and R, this method can be implemented using the particle filter SMC algorithm.\nFinally, the bootstrap filter is a simple and flexible technique used to estimate the state of a system based on a set of measurements. It works by resampling the measurements and updating the estimates based on the new sample. This method is often used in finance to estimate the value of financial instruments such as options and futures.\nThroughout this chapter, we will provide code examples in both Python and R to demonstrate the implementation of these approximation methods. We will also discuss the advantages and disadvantages of each method and provide guidance on when to use each technique.\nMonte Carlo Markov Chain methods are discussed in the next chapter"
  },
  {
    "objectID": "Gaussian_sum_filter_.html",
    "href": "Gaussian_sum_filter_.html",
    "title": "4  Gaussian sum filter",
    "section": "",
    "text": "5 Gaussian sum filter\nThe Gaussian sum filter is a technique used to approximate probability distributions that can be represented as a sum of Gaussian distributions. It is often used in signal processing to estimate the state of a system from noisy measurements. The approach involves representing the probability density function (PDF) of the system state as a sum of Gaussian distributions with different means and covariance matrices. The resulting approximation is then used to estimate the state of the system.\n# Generate some example data\nset.seed(123)\nx &lt;- seq(0, 10, length.out = 101)\ny &lt;- sin(x) + rnorm(101, 0, 0.1)\n\n\n\n# Define the Gaussian sum filter function\ngsf &lt;- function(x, y, mu, sigma, w) {\n  # Compute the Gaussian kernels\n  kernels &lt;- sapply(mu, function(m) dnorm(x, m, sigma))\n  \n  # Compute the filtered values\n  filtered &lt;- apply(kernels * w, 2, function(wk) sum(wk * y))\n  \n  return(filtered)\n}\n\n# Define the parameters of the filter\nmu &lt;- seq(0, 10, length.out = 101)\nsigma &lt;- 0.5\nw &lt;- rep(1/11, 101)\n\n# Apply the filter to the example data\nfiltered &lt;- gsf(x, y, mu, sigma, w)\n\n\n# Plot the original and filtered data\ndf_plot &lt;- data.frame(x =x, Original = y, Filtered = filtered)\n\n\nplot(x, y, type = 'l', col = 'blue', main = 'Gaussian sum filter example')\nlines(x, filtered, col = 'red')\nlegend('topleft', legend = c('Original', 'Filtered'), col = c('blue', 'red'), lty = 1)\nIn this example, we generate some example data (y) and apply a Gaussian sum filter to it using the gsf function. The filter is defined by the parameters mu (the means of the Gaussian kernels), sigma (the standard deviation of the kernels), and w (the weights of the kernels). We then plot the original and filtered data to visualize the effect of the filter."
  },
  {
    "objectID": "Gaussian_sum_filter_.html#load-libraries-needed",
    "href": "Gaussian_sum_filter_.html#load-libraries-needed",
    "title": "4  Gaussian sum filter",
    "section": "4.1 load libraries needed",
    "text": "4.1 load libraries needed\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "Kalman_filter_.html",
    "href": "Kalman_filter_.html",
    "title": "5  Kalman filter",
    "section": "",
    "text": "6 Kalman filter\nthe Kalman filter is an algorithm that estimates the state of a dynamic system in the presence of noise. It is widely used in applications such as navigation, control systems, and signal processing. The Kalman filter assumes that the system dynamics can be modeled using a linear state-space model, and the noise is Gaussian and white. It recursively updates the estimate of the state based on the current measurement and the previous estimate.\nHere’s an example of how to implement a simple Kalman filter in R to estimate the position of a moving object:\n# Generate some example data\nset.seed(123)\nn &lt;- 100\nx_true &lt;- 1:n\nx_meas &lt;- x_true + rnorm(n, sd = 5)\n\n# Define the Kalman filter function\nkalman_filter &lt;- function(x_meas, Q, R, x_init, P_init) {\n  n &lt;- length(x_meas)\n  x_est &lt;- rep(0, n)\n  P_est &lt;- rep(0, n)\n  K &lt;- rep(0, n)\n  \n  x_est[1] &lt;- x_init\n  P_est[1] &lt;- P_init\n  \n  for (i in 2:n) {\n    # Predict step\n    x_pred &lt;- x_est[i-1]\n    P_pred &lt;- P_est[i-1] + Q\n    \n    # Update step\n    K[i] &lt;- P_pred / (P_pred + R)\n    x_est[i] &lt;- x_pred + K[i] * (x_meas[i] - x_pred)\n    P_est[i] &lt;- (1 - K[i]) * P_pred\n  }\n  \n  return(list(x_est = x_est, P_est = P_est, K = K))\n}\n\n# Define the parameters of the filter\nQ &lt;- 0.1\nR &lt;- 25\nx_init &lt;- 0\nP_init &lt;- 10\n\n# Apply the filter to the example data\nkf_result &lt;- kalman_filter(x_meas, Q, R, x_init, P_init)\n\n# kf_result\n\n# Plot the results\nplot(x_true, type = 'l', col = 'blue', ylim = c(0, n+1), main = 'Kalman filter example')\nlines(x_meas, col = 'red')\nlines(kf_result$x_est, col = 'green')\nlegend('topleft', legend = c('True', 'Measured', 'Estimated'), col = c('blue', 'red', 'green'), lty = 1)\nIn this example, we generate some example data representing the true position of a moving object (x_true) and noisy measurements of its position (x_meas). We then define a Kalman filter function (kalman_filter) that takes as input the measurements and some parameters (Q, R, x_init, and P_init) and returns the estimated position (x_est), the estimated uncertainty (P_est), and the Kalman gain (K) at each time step. We apply the filter to the example data and plot the results, showing the true position in blue, the measured position in red, and the estimated position in green."
  },
  {
    "objectID": "Kalman_filter_.html#load-libraries-needed",
    "href": "Kalman_filter_.html#load-libraries-needed",
    "title": "5  Kalman filter",
    "section": "5.1 load libraries needed",
    "text": "5.1 load libraries needed\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "Particle_filter___Sequential_Monte_Carlo_.html",
    "href": "Particle_filter___Sequential_Monte_Carlo_.html",
    "title": "6  Particle filter - Sequential Monte Carlo (SMC)",
    "section": "",
    "text": "7 R\n# Particle Filter SMC in R\n\n# Function to generate observations\ngenerate_observations &lt;- function(time_steps, true_states) {\n  observations &lt;- rnorm(time_steps, true_states, 1)\n  return(observations)\n}\n\n# Function to calculate the likelihood of a particle\ncalculate_likelihood &lt;- function(particle, observation) {\n  likelihood &lt;- dnorm(observation, particle, 1)\n  return(likelihood)\n}\n\n# Particle Filter SMC function\nparticle_filter_smc &lt;- function(time_steps, num_particles) {\n  # Initialize particles and weights\n  particles &lt;- rnorm(num_particles, 0, 1)\n  weights &lt;- rep(1/num_particles, num_particles)\n  \n  # Initialize estimates\n  filtered_states &lt;- numeric(time_steps)\n  \n  for (t in 1:time_steps) {\n    # Generate observations\n    true_state &lt;- sin(t/5) # True state at time t\n    observation &lt;- generate_observations(1, true_state)\n    \n    # Update weights\n    for (i in 1:num_particles) {\n      weights[i] &lt;- calculate_likelihood(particles[i], observation) * weights[i]\n    }\n    weights &lt;- weights / sum(weights)\n    \n    # Resampling step\n    indices &lt;- sample.int(num_particles, num_particles, replace = TRUE, prob = weights)\n    particles &lt;- particles[indices]\n    \n    # Estimate filtered state\n    filtered_state &lt;- mean(particles)\n    filtered_states[t] &lt;- filtered_state\n  }\n  \n  return(filtered_states)\n}\n\n# Run the particle filter\nset.seed(123)\ntime_steps &lt;- 100\nnum_particles &lt;- 100\nfiltered_states &lt;- particle_filter_smc(time_steps, num_particles)\n\n# Plot the results\nplot(filtered_states, type = \"l\", main = \"Particle Filter SMC in R\")\nThe code generates observations from a known true state and uses the particle filter to estimate the filtered states based on these observations.\nThe R code uses the rnorm function to generate normally distributed observations and likelihood calculations. It initializes particles and weights, updates weights based on the likelihood, performs resampling, and estimates the filtered state using the mean of the particles. The results are plotted using the plot function."
  },
  {
    "objectID": "Particle_filter___Sequential_Monte_Carlo_.html#load-libraries-needed",
    "href": "Particle_filter___Sequential_Monte_Carlo_.html#load-libraries-needed",
    "title": "6  Particle filter - Sequential Monte Carlo (SMC)",
    "section": "6.1 load libraries needed",
    "text": "6.1 load libraries needed\n\nlibrary(ggplot2)\n\nParticle Filter Sequential Monte Carlo (SMC) is a recursive Bayesian filtering algorithm used to estimate the hidden state of a system based on noisy observations. It is particularly useful in situations where the system dynamics are nonlinear and non-Gaussian.\nThe SMC particle filter represents the posterior distribution of the hidden state using a set of weighted particles. Each particle corresponds to a hypothesis about the hidden state, and its weight represents the likelihood of that hypothesis given the observations.\nThe algorithm proceeds in a sequential manner, updating the particles and their weights at each time step:\n1 - Initialization: Start with an initial set of particles representing the prior distribution of the hidden state.\n2 - Prediction: Propagate each particle forward in time according to the system dynamics, introducing stochasticity if necessary.\n3 - Update: Compute the likelihood of each particle by comparing its predicted state to the observed data. Assign weights to the particles based on their likelihoods.\n4 - Resampling: Resample particles with replacement from the weighted set, favoring particles with higher weights. This step eliminates low-weight particles and duplicates high-weight particles, ensuring a diverse set of particles that represents the posterior distribution more accurately.\n5 - Estimation: Estimate the filtered state by summarizing the set of particles, commonly using the mean or weighted average.\nRepeat steps 2 to 5 for each subsequent time step.\nBy iteratively updating and resampling the particles, the SMC particle filter provides an approximation of the posterior distribution of the hidden state over time. It is a flexible and powerful technique for state estimation in dynamic systems with nonlinear and non-Gaussian characteristics."
  },
  {
    "objectID": "Multilevel_Particle_filter___Sequential_Monte_Carlo_.html",
    "href": "Multilevel_Particle_filter___Sequential_Monte_Carlo_.html",
    "title": "7  Multilevel Particle filter - SMC",
    "section": "",
    "text": "8 Multi-level models\nYes, the SMC particle filter can be used for multilevel models. Multilevel models involve hierarchical structures where the observations are nested within different levels or groups. The SMC particle filter can handle such structures by appropriately accounting for the covariance between levels.\nIn the context of multilevel models, the SMC particle filter extends the standard particle filtering algorithm by introducing additional steps to account for the covariance between levels. Here’s a high-level overview of how the SMC particle filter can be applied to multilevel models:\n1 - Initialization: Initialize the particles at each level of the multilevel model, taking into account the prior distributions and covariances between levels.\n2 - Prediction: Propagate the particles forward in time at each level based on the system dynamics and the covariance structures within and between levels.\n3 - Update: Calculate the likelihood of the observations at each level, considering the covariance between levels. Assign weights to the particles based on their likelihoods.\n4 - Resampling: Perform resampling at each level independently, ensuring that the resampling step maintains the covariance structure between levels. This can be achieved by using techniques such as conditional resampling or resampling based on auxiliary variables that capture the dependencies between levels.\n5 - Estimation: Estimate the filtered states and parameters at each level based on the particles, taking into account the covariance between levels. This can be done by summarizing the particles using appropriate statistics or using more advanced estimation techniques such as particle learning or Markov chain Monte Carlo (MCMC).\nBy explicitly considering the covariance structure between levels, the SMC particle filter allows for inference in multilevel models. The resampling step, in particular, plays a crucial role in maintaining the dependencies between levels, ensuring that the particles represent the posterior distribution accurately.\nIt’s worth noting that the implementation of the SMC particle filter for multilevel models can be more complex than for single-level models, as it requires carefully handling the covariance structures. However, with appropriate adjustments and considerations, the SMC particle filter can be a powerful tool for state estimation in multilevel models.\nHere’s a toy model that demonstrates the application of the SMC particle filter to a simple multilevel model. In this example, we’ll consider a two-level model with a common parameter that affects the observations at each level.\nIn this toy model, we generate observations at two levels. The observations at the first level are affected by a common parameter, and the observations at the second level depend on the observations at the first level. The particle filter estimates the filtered states and provides a distribution of the parameter values over time.\nFeel free to modify the parameters or add complexity to the model to explore different scenarios and observe how the SMC particle filter captures the dependencies between levels in the multilevel model. # R\n# Generate observations at each level\ngenerate_observations &lt;- function(num_samples, true_param, noise_sd) {\n  observations &lt;- matrix(nrow = num_samples, ncol = 2)\n  for (i in 1:num_samples) {\n    # Level 1 observations\n    level1_obs &lt;- rnorm(1, true_param, noise_sd)\n    \n    # Level 2 observations\n    level2_obs &lt;- rnorm(1, level1_obs, noise_sd)\n    \n    observations[i, ] &lt;- c(level1_obs, level2_obs)\n  }\n  return(observations)\n}\n\n# Calculate the likelihood of a particle at each level\ncalculate_likelihood &lt;- function(particle, observation, noise_sd) {\n  level1_likelihood &lt;- dnorm(observation[1], particle, noise_sd)\n  level2_likelihood &lt;- dnorm(observation[2], observation[1], noise_sd)\n  return(level1_likelihood * level2_likelihood)\n}\n\n# Particle Filter SMC function for multilevel model\nparticle_filter_smc_multilevel &lt;- function(num_samples, num_particles, noise_sd) {\n  # Initialize particles and weights\n  particles &lt;- rnorm(num_particles, 0, 1)\n  weights &lt;- rep(1/num_particles, num_particles)\n  \n  # Initialize estimates\n  filtered_states &lt;- matrix(nrow = num_samples, ncol = num_particles)\n  \n  for (t in 1:num_samples) {\n    # Generate observations\n    observations &lt;- generate_observations(1, true_param, noise_sd)\n    \n    # Update weights\n    for (i in 1:num_particles) {\n      weights[i] &lt;- calculate_likelihood(particles[i], observations, noise_sd) * weights[i]\n    }\n    weights &lt;- weights / sum(weights)\n    \n    # Resampling step\n    indices &lt;- sample.int(num_particles, num_particles, replace = TRUE, prob = weights)\n    particles &lt;- particles[indices]\n    \n    # Estimate filtered state\n    filtered_states[t, ] &lt;- particles\n  }\n  \n  return(filtered_states)\n}\n\n# True parameter value\ntrue_param &lt;- 2.0\n\n# Set random seed for reproducibility\nset.seed(123)\n\n# Run the particle filter for the multilevel model\nnum_samples &lt;- 100\nnum_particles &lt;- 100\nnoise_sd &lt;- 1.0\nfiltered_states &lt;- particle_filter_smc_multilevel(num_samples, num_particles, noise_sd)\n\n# Plot the results\nmatplot(filtered_states, type = \"l\", lty = 1, col = \"blue\", main = \"Particle Filter SMC for Multilevel Model\",\n        xlab = \"Time Step\", ylab = \"Filtered State\")\nabline(h = true_param, col = \"red\", lty = 2, lwd = 2, label = \"True Parameter\")\nlegend(\"topright\", legend = \"True Parameter\", col = \"red\", lty = 2, lwd = 2)"
  },
  {
    "objectID": "Multilevel_Particle_filter___Sequential_Monte_Carlo_.html#load-libraries-needed",
    "href": "Multilevel_Particle_filter___Sequential_Monte_Carlo_.html#load-libraries-needed",
    "title": "7  Multilevel Particle filter - SMC",
    "section": "7.1 load libraries needed",
    "text": "7.1 load libraries needed\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "Bootstrap_filter_.html",
    "href": "Bootstrap_filter_.html",
    "title": "8  Bootstrap filter (To Be Continued)",
    "section": "",
    "text": "9"
  },
  {
    "objectID": "Bootstrap_filter_.html#load-libraries-needed",
    "href": "Bootstrap_filter_.html#load-libraries-needed",
    "title": "8  Bootstrap filter (To Be Continued)",
    "section": "8.1 load libraries needed",
    "text": "8.1 load libraries needed\n\nlibrary(rstanarm)\nlibrary(ggplot2)\nlibrary(bayesplot)\nlibrary(blavaan)\nlibrary(dplyr)\nlibrary(magick)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(ggridges)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(rsvg)\ntheme_set(bayesplot::theme_default())\ntheme_set(theme_bw())"
  },
  {
    "objectID": "Markov_Chain_Monte_Carlo_methods_.html#load-libraries-needed",
    "href": "Markov_Chain_Monte_Carlo_methods_.html#load-libraries-needed",
    "title": "Markov Chain Monte Carlo methods (To Be Continued)",
    "section": "load libraries needed",
    "text": "load libraries needed\n\nlibrary(rstanarm)\nlibrary(ggplot2)\nlibrary(bayesplot)\nlibrary(blavaan)\nlibrary(dplyr)\nlibrary(magick)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(ggridges)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(rsvg)\ntheme_set(bayesplot::theme_default())\ntheme_set(theme_bw())"
  },
  {
    "objectID": "Metropolis_Hastings_algorithm_.html",
    "href": "Metropolis_Hastings_algorithm_.html",
    "title": "9  Metropolis Hastings algorithm (To Be Continued)",
    "section": "",
    "text": "10"
  },
  {
    "objectID": "Metropolis_Hastings_algorithm_.html#load-libraries-needed",
    "href": "Metropolis_Hastings_algorithm_.html#load-libraries-needed",
    "title": "9  Metropolis Hastings algorithm (To Be Continued)",
    "section": "9.1 load libraries needed",
    "text": "9.1 load libraries needed\n\nlibrary(rstanarm)\n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.21.4\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\nlibrary(ggplot2)\nlibrary(bayesplot)\n\nThis is bayesplot version 1.10.0\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\nlibrary(blavaan)\n\nThis is blavaan 0.4-7\n\n\nOn multicore systems, we suggest use of future::plan(\"multicore\") or\n  future::plan(\"multisession\") for faster post-MCMC computations.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.12.3\nEnabled features: cairo, freetype, fftw, ghostscript, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fontconfig, x11\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n✔ readr     2.1.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggridges)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(rsvg)\n\nLinking to librsvg 2.48.8\n\ntheme_set(bayesplot::theme_default())\ntheme_set(theme_bw())"
  },
  {
    "objectID": "Gibbs_sampling_.html",
    "href": "Gibbs_sampling_.html",
    "title": "10  Gibbs sampling (To Be Continued)",
    "section": "",
    "text": "11"
  },
  {
    "objectID": "Gibbs_sampling_.html#load-libraries-needed",
    "href": "Gibbs_sampling_.html#load-libraries-needed",
    "title": "10  Gibbs sampling (To Be Continued)",
    "section": "10.1 load libraries needed",
    "text": "10.1 load libraries needed\n\nlibrary(rstanarm)\n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.21.4\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\nlibrary(ggplot2)\nlibrary(bayesplot)\n\nThis is bayesplot version 1.10.0\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\nlibrary(blavaan)\n\nThis is blavaan 0.4-7\n\n\nOn multicore systems, we suggest use of future::plan(\"multicore\") or\n  future::plan(\"multisession\") for faster post-MCMC computations.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.12.3\nEnabled features: cairo, freetype, fftw, ghostscript, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fontconfig, x11\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n✔ readr     2.1.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggridges)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(rsvg)\n\nLinking to librsvg 2.48.8\n\ntheme_set(bayesplot::theme_default())\ntheme_set(theme_bw())"
  },
  {
    "objectID": "Hamiltonian_Monte_Carlo_.html",
    "href": "Hamiltonian_Monte_Carlo_.html",
    "title": "11  Hamiltonian monte carlo (To Be Continued)",
    "section": "",
    "text": "12"
  },
  {
    "objectID": "Hamiltonian_Monte_Carlo_.html#load-libraries-needed",
    "href": "Hamiltonian_Monte_Carlo_.html#load-libraries-needed",
    "title": "11  Hamiltonian monte carlo (To Be Continued)",
    "section": "11.1 load libraries needed",
    "text": "11.1 load libraries needed\n\nlibrary(rstanarm)\n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.21.4\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\nlibrary(ggplot2)\nlibrary(bayesplot)\n\nThis is bayesplot version 1.10.0\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\nlibrary(blavaan)\n\nThis is blavaan 0.4-7\n\n\nOn multicore systems, we suggest use of future::plan(\"multicore\") or\n  future::plan(\"multisession\") for faster post-MCMC computations.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.12.3\nEnabled features: cairo, freetype, fftw, ghostscript, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fontconfig, x11\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n✔ readr     2.1.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggridges)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(rsvg)\n\nLinking to librsvg 2.48.8\n\ntheme_set(bayesplot::theme_default())\ntheme_set(theme_bw())"
  },
  {
    "objectID": "Concluding___Comparison_.html#load-libraries-needed",
    "href": "Concluding___Comparison_.html#load-libraries-needed",
    "title": "Concluding, Comparison (To Be Continued)",
    "section": "load libraries needed",
    "text": "load libraries needed\n\nlibrary(rstanarm)\n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.21.4\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\nlibrary(ggplot2)\nlibrary(bayesplot)\n\nThis is bayesplot version 1.10.0\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\nlibrary(blavaan)\n\nThis is blavaan 0.4-7\n\n\nOn multicore systems, we suggest use of future::plan(\"multicore\") or\n  future::plan(\"multisession\") for faster post-MCMC computations.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.12.3\nEnabled features: cairo, freetype, fftw, ghostscript, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fontconfig, x11\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n✔ readr     2.1.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggridges)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(rsvg)\n\nLinking to librsvg 2.48.8\n\ntheme_set(bayesplot::theme_default())\ntheme_set(theme_bw())"
  },
  {
    "objectID": "The_Bayes_filter___Variance_influence_.html#when-does-the-prior-start-to-influence-the-posterior-too-much",
    "href": "The_Bayes_filter___Variance_influence_.html#when-does-the-prior-start-to-influence-the-posterior-too-much",
    "title": "2  Bayes filter - Variance influence",
    "section": "2.1 When does the prior start to influence the posterior too much?",
    "text": "2.1 When does the prior start to influence the posterior too much?\nFor this analysis I chose a prior that is slightly closer to the population then in the previous section, for plotting purposes. The prior will be sharp enough to demonstrate the influence of the prior on the posterior.\nHere I will keep the mean of the prior the same and change the precision (inverse variance). A list will be made with all the posterior distributions.\n\n## create the sample (data)\nset.seed(1234)\nn &lt;- 100\nx0 &lt;- rnorm(n, 12, 2)\nb0 = 1\ny = b0 * x0\ndf = data.frame(y, x0)\n\n\nvariance &lt;- c(1E-5, 1E-4, 1E-3, 1E-2, 1E-1, 0.5E-1, 0.4E-1, 0.3E-1, 0.2E-1,0.19E-1,0.18E-1,0.17E-1,0.16E-1,0.15E-1,0.14E-1,0.13E-1,0.12E-1,0.11E-1,0.1111E-1,0.111111E-1, 1, 1E1, 1E2, 1E3, 1E4, 1E5)\n# variance &lt;- c(1E-2, 1E-1, 1, 2, 3, 4, 5, 7, 9, 10)\nlist_posterior_test &lt;-list()\nlist_priors_test &lt;- list()\ncount_i &lt;- 0\nfor(i in variance){\n  \n  count_i &lt;- count_i +1\n  list_priors_test[count_i] &lt;- as.array(rnorm(100, 30, i))\n  fit_test &lt;- stan_glm(y ~ 1, \n                         data = df, \n                         prior_intercept = normal(c(30), i), \n                         cores = parallel::detectCores(), \n                         seed = 1234)\n  if(i == variance[1]){\n    list_posterior_test[1] &lt;- as.array(fit_test)\n  }\n  else{\n  list_posterior_test[[length(list_posterior_test) + 1]] &lt;- as.array(fit_test)\n  }\n}\n\n\nhist(y, main = 'sample distribution')\n\n\n\nmcmc_dens_overlay(list_posterior_test[[11]])\n\n\n\nmcmc_dens(list_posterior_test[[2]])\n\n\n\nmeans_test &lt;- array(NA, dim = length(variance))\nfor(j in 1:length(variance)){\n  if(j ==1){\n    means_test[j] &lt;- mean(list_posterior_test[[j]])\n  }\n  else{\n    data_post &lt;- mcmc_dens_chains_data(list_posterior_test[[j]])\n    data_post_sub &lt;- subset(data_post, parameter == '(Intercept)')\n    means_test[j] &lt;- mean(data_post_sub$x)\n  }\n}\nplot(log10(variance), means_test)\n\n\n\ndf_plots &lt;- data.frame(means_test, variance)\nggplot(df_plots, aes(log10(variance), means_test))+\n  geom_point() +\n  geom_line()\n\n\n\n\nWe see here that a prior with a width of 1 and higher is uninformative enough in relation to the data that the model can still find the correct answer."
  },
  {
    "objectID": "The_Bayes_filter___Variance_influence_.html#look-at-the-ratio-of-the-location-vs-the-width-of-the-prior-n-500",
    "href": "The_Bayes_filter___Variance_influence_.html#look-at-the-ratio-of-the-location-vs-the-width-of-the-prior-n-500",
    "title": "2  Bayes filter - Variance influence",
    "section": "2.2 Look at the ratio of the location vs the width of the prior n = 500",
    "text": "2.2 Look at the ratio of the location vs the width of the prior n = 500\n\nstart_time &lt;- Sys.time()\n\nvariance &lt;- c(1E-4, 1E-3, 1E-2, 1E-1, 0.5E-1, 0.11E-1, 1, 1.5, 1E1, 1E2, 1E3, 1E4)\nmean_prior &lt;- c(-10, -5, -1, 0, 1, 5, 10, 50, 100, 500 )\nlen &lt;- length(variance)*length(mean_prior)\n\nlist_posterior_test &lt;-list()\nlist_priors_test &lt;- list()\nvar_mean_ratio &lt;- array(NA, dim = len)\ncount_j &lt;- 0\ncount_i &lt;- 0\nfor(j in mean_prior){\n  \n  count_j &lt;- count_j+1\n  \n  for(i in variance){\n    \n    count_i &lt;- count_i + 1\n    list_priors_test[count_i] &lt;- as.array(rnorm(100, j, i))\n    \n    fit_test &lt;- stan_glm(y ~ 1, \n                           data = df, \n                           prior_intercept = normal(c(j), i), \n                           cores = parallel::detectCores(), \n                           seed = 1234)\n    \n    if(i == variance[1]){\n      list_posterior_test[count_i] &lt;- as.array(fit_test)\n    }\n    else{\n    list_posterior_test[[length(list_posterior_test) + 1]] &lt;- as.array(fit_test)\n    }\n    \n    var_mean_ratio[count_i] &lt;- i/j\n  }\n}\n\nhist(y, main = 'sample distribution')\n\n\n\nmcmc_dens_overlay(list_posterior_test[[11]])\n\n\n\nmeans_test &lt;- array(NA, dim = len)\nexception &lt;- 0\nfor(k in 1:len){\n  if(k == 1 + exception*12){\n    exception &lt;- exception + 1\n    means_test[k] &lt;- as.array(mean(list_posterior_test[[k]]))\n  }\n  else{\n      data_post &lt;- mcmc_dens_chains_data(list_posterior_test[[k]])\n      data_post_sub &lt;- subset(data_post, parameter == '(Intercept)')\n      means_test[k] &lt;- as.array(mean(data_post_sub$x))\n  }\n}\n\n\n\n#plot the ratio of the width/mean of prior vs means of posterior\ndf_plots_2 &lt;- data.frame(means_test, variance, var_mean_ratio)\nggplot(df_plots_2, aes(var_mean_ratio, log(means_test))) +\n  geom_point() \n\n\n\n  #+ # geom_line()\n  \nstop_time &lt;- Sys.time()\n(time_elapsed &lt;- start_time - stop_time)\n\nTime difference of -24.91171 mins\n\n\n\n2.2.1 plotting the prior, posterior and data\n\n## create the sample (data)\nset.seed(1234)\nn &lt;- 1000\nx0 &lt;- rnorm(n, 15, 2)\nb0 = 1\ny = b0 * x0\ndf = data.frame(y, x0)\n\n# Design the different priors -&gt; in this case only the variance changes\nvariance &lt;- c(seq(1,2,0.1), seq(2,5, 0.5), seq(5,10,1), seq(10,100,10))\n\n# initialise the list for plotting the distributions later on\nlist_posterior_test &lt;-list()\nlist_priors_test &lt;- list()\n\n#index\ncount_i &lt;- 0\n\n#fit the model and produce the posterior, save both posterior and corresponding prior\nfor(i in variance){\n  count_i &lt;- count_i +1\n  list_priors_test[[count_i]] &lt;- as.array(rnorm(1000, 50, i))\n  fit_test &lt;- stan_glm(y ~ 1, \n                         data = df, \n                         prior_intercept = normal(c(50), i), \n                         cores = parallel::detectCores(), \n                         seed = 1234)\n  if(i == variance[1]){\n    list_posterior_test[[1]] &lt;- as.array(fit_test)\n  }\n  else{\n  list_posterior_test[[length(list_posterior_test) + 1]] &lt;- as.array(fit_test)\n  }\n}\n\n\n#################################### Magick ########################################################\n\n\n## create a directory to which the images will be written\n# dir_out &lt;- file.path(\"D:\\\\UU\\\\ResearchMnS\\\\Git_repo\\\\BSUA-CHM\\\\Pre_research_R_files\\\\Figures_for_gif\", \"post_prior_data\")\ndir_out &lt;- file.path(getwd(), \"post_prior_data\")\ndir.create(dir_out, recursive = TRUE)\n\n## prepare data; done before. Start with just gif with changing posterior\npost_dist &lt;- data.frame(list_posterior_test)\nprior_dist &lt;- list_priors_test\nsample_dist &lt;- y\n\n# --------------------------------------------------------------------------------------------------\n## loop through distributions ...\n## subset data ...\n## create density plot of posterior/prior/sample\n## write plot to file\n\nfor(index in 1:length(variance)){\n  if(index == 1){\n    name &lt;- \"chain.1..Intercept.\"\n  }\n  else{\n    name &lt;- paste(c(\"chain.1..Intercept..\", index-1), collapse = \"\")\n  }\n  \n  p &lt;- ggplot(post_dist) +\n          geom_density(aes(x = eval(as.name(name))), color = 1, fill = 1, alpha = 0.25) +\n          # geom_histogram(aes(x = sample_dist), color = 2, fill = 2, alpha = 0.25)+\n          geom_density(aes(x = sample_dist), color = 2, fill = 2, alpha = 0.25) +\n          geom_density(aes(x = prior_dist[[index]]), color = 3, fill = 3, alpha = 0.25) +\n          # scale_y_continuous(limits = c(0, 5), breaks = seq(0,1e5, by = 5000)) +\n          # theme_minimal()\n          theme_classic() +\n          xlim(10, 55) + \n          ylim(0, 0.8) +\n          labs(x = \"Param value\", y = \"Prob\", title = index)\n  \n  fp &lt;- file.path(dir_out, paste0(index, \".png\"))\n  \n  ggsave(plot = p,\n         filename = fp,\n         device = \"png\")\n}\n\n\n\n## list file names and read in\nimgs &lt;- list.files(dir_out, full.names = TRUE)\nimg_list &lt;- lapply(imgs, image_read)\n\n## join the images together\nimg_joined &lt;- image_join(img_list)\n\n## animate at 2 frames per second\nimg_animated &lt;- image_animate(img_joined, fps = 2, optimize = TRUE)\n\n\n## save to disk\nimage_write(image = img_animated,\n            path = \"post_prior_data.gif\")\n\nplay_image &lt;- TRUE\nif(play_image){\n  ## view animated image\n  img_animated\n}"
  },
  {
    "objectID": "The_Bayes_filter___Variance_influence_.html#variance-study---retry-above-in-a-different-format.-the-data-structure-above-is-quite-unorganised.",
    "href": "The_Bayes_filter___Variance_influence_.html#variance-study---retry-above-in-a-different-format.-the-data-structure-above-is-quite-unorganised.",
    "title": "2  Bayes filter - Variance influence",
    "section": "2.3 Variance study - Retry above in a different format. The data structure above is quite unorganised.",
    "text": "2.3 Variance study - Retry above in a different format. The data structure above is quite unorganised.\nLets make a dataframe of each iteration and put this in a list. Then use this list to create the plots later. The making of the plots in a different loop makes that the program is more easily adjustable. If we want to make changes in the plots we don’t have to run the data frame creation\n\n## create the sample (data)\nset.seed(1234)\nn &lt;- 100\nx0 &lt;- rnorm(n, 12, 2)\nb0 = 1\ny = b0 * x0\ndf = data.frame(y, x0)\n\n# Design the different priors -&gt; in this case only the variance changes\nvariance &lt;- c(seq(1E-2,1E-1,2E-2),seq(1E-1, 1, 2E-1),seq(1,2,5E-2),seq(2,6,2),seq(6,106, 25))\n\n# initialise the list for plotting the distributions later on\nlist_posterior &lt;-list()\nlist_prior &lt;- list()\n\n#index\ncount_i &lt;- 0\n\n#fit the model and produce the posterior, save both posterior and corresponding prior\nfor(i in variance){\n  count_i &lt;- count_i +1\n  list_prior[[count_i]] &lt;- as.array(rnorm(2000, 50, i))\n  fit_test &lt;- stan_glm(y ~ 1, \n                         data = df, \n                         prior_intercept = normal(c(50), i), \n                         cores = parallel::detectCores(), \n                         seed = 1234)\n  \n  draws &lt;- as.data.frame(fit_test)\n  list_posterior[[count_i]] &lt;- draws$`(Intercept)`\n  \n}\n\n## prep the data \n\nlist_dist &lt;- list()\n\nfor(index in 1:length(variance)){\n\n  a &lt;- list_posterior[[index]]\n  \n  b &lt;- list_prior[[index]]\n  \n  c &lt;- y \n  \n  ab_ &lt;- append(a,b)\n  abc_ &lt;- append(ab_, c)\n  \n  df_dist &lt;- data.frame(abc_)\n  df_dist &lt;- data.frame(unlist(df_dist))\n  df_dist$group &lt;- c(rep('posterior', length(a)), rep('prior', length(b)), rep('sample', length(c)))\n  list_dist[[index]] &lt;- df_dist\n\n}\n\n\n2.3.1 Ridgeline plots - Create plots and .gif from the results\n\n## create a directory to which the images will be written\ndir_out &lt;- file.path(getwd(), \"post_prior_data_ridge\")\ndir.create(dir_out, recursive = TRUE)\n\n## Create the plots with ridgelines \n\nfor(j in 1:length(variance)){\n  \n  ## Creating the plots\n  # p &lt;- ggplot(list_dist[[j]], aes(x = unlist.df_dist., y = group, fill = group)) +\n  #           stat_density_ridges(quantile_lines = TRUE, quantiles = 0.5) +\n  #           labs(x = \"Param value\", y = \"Prob\", title = variance[j]) +\n  #           xlim(10, 55) +\n  #           theme(legend.position = \"none\")\n\n    \n  p &lt;-  ggplot(list_dist[[j]], aes(x = unlist.df_dist., y = group, fill = 0.5 - abs(0.5 - stat(ecdf)))) +\n            stat_density_ridges(geom = \"density_ridges_gradient\", calc_ecdf = TRUE) +\n            scale_fill_viridis_c(name = \"Tail probability\", direction = -1)+\n            theme(legend.position = \"none\") +\n            xlim(10, 55) +\n            labs(x = \"Param value\", y = \"Count\", title = variance[j])\n  \n    fp &lt;- file.path(dir_out, paste(stringr::str_pad(j, 4, pad = \"0\"), \".png\", sep=\"\"))\n  \n    ggsave(plot = p,\n           filename = fp,\n           device = \"png\")\n}\n\n\n# list file names and read in\nimgs &lt;- list.files(dir_out, full.names = TRUE)\n# imgs &lt;- list.files(dir_out)\nimg_list &lt;- lapply(imgs, image_read)\n\n## join the images together\nimg_joined &lt;- image_join(img_list)\n\n## animate at 2 frames per second\nimg_animated &lt;- image_animate(img_joined, fps = 2, optimize = FALSE)\n\n\n## save to disk\nimage_write(image = img_animated,\n            path = \"post_prior_data_ridge.gif\")\n\nplay_image &lt;- TRUE\nif(play_image){\n  ## view animated image\n  img_animated}\n\n\n\n\n\n\n2.3.2 Histogram plots - Create the plots with histograms\n\ndir_out &lt;- file.path(getwd(), \"post_prior_data_hist\")\ndir.create(dir_out, recursive = TRUE)\n\nfor(j in 1:length(variance)){\n  \n  ## Creating the plots\n  p &lt;- ggplot(list_dist[[j]], aes(x = unlist.df_dist., fill = group )) +\n            geom_histogram() +\n            # ylim(0, 0.8) +\n            xlim(10, 55) +\n            theme_light() +\n            labs(x = \"Param value\", y = \"Count\", title = variance[j])\n  \n    fp &lt;- file.path(dir_out, paste(stringr::str_pad(j, 4, pad = \"0\"), \".png\", sep=\"\"))\n  \n    ggsave(plot = p,\n           filename = fp,\n           device = \"png\")\n}\n\n# list file names and read in\nimgs &lt;- list.files(dir_out, full.names = TRUE)\nimg_list &lt;- lapply(imgs, image_read)\n\n## join the images together\nimg_joined &lt;- image_join(img_list)\n\n## animate at 2 frames per second\nimg_animated &lt;- image_animate(img_joined, fps = 2, optimize = TRUE)\n\n\n## save to disk\nimage_write(image = img_animated,\n            path = \"post_prior_data_hist.gif\")\n\nplay_image &lt;- TRUE\nif(play_image){\n  ## view animated image\n  img_animated}\n\n\n\n\n\n\n2.3.3 Density plots - Create the plots with Density plots\n\ndir_out &lt;- file.path(getwd(), \"post_prior_data_dens\")\ndir.create(dir_out, recursive = TRUE)\n\nfor(j in 1:length(variance)){\n  \n  ## Creating the plots\n  p &lt;- ggplot(list_dist[[j]], aes(x = unlist.df_dist., fill = group )) +\n            geom_density(alpha = .5) +\n            ylim(0, 0.8) +\n            xlim(10, 55) +\n            theme_light() +\n            labs(x = \"Param value\", y = \"Count\", title = variance[j])\n  \n    fp &lt;- file.path(dir_out, paste(stringr::str_pad(j, 4, pad = \"0\"), \".png\", sep=\"\"))\n  \n    ggsave(plot = p,\n           filename = fp,\n           device = \"png\")\n}\n\n# list file names and read in\nimgs &lt;- list.files(dir_out, full.names = TRUE)\nimg_list &lt;- lapply(imgs, image_read)\n\n## join the images together\nimg_joined &lt;- image_join(img_list)\n\n## animate at 2 frames per second\nimg_animated &lt;- image_animate(img_joined, fps = 2, optimize = TRUE)\n\n\n## save to disk\nimage_write(image = img_animated,\n            path = \"post_prior_data_dens.gif\")\n\n\n\nplay_image &lt;- TRUE\nif(play_image){\n  ## view animated image\n  img_animated}"
  }
]